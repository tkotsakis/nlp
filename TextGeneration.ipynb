{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I adapted [this notebook](https://github.com/Elucidation/Ngram-Tutorial/blob/master/NgramTutorial.ipynb) to Python 3. \n",
    "\n",
    "## IPython Notebook - N-gram Tutorial\n",
    "\n",
    "Here is the explanation from the original author:\n",
    "\n",
    "_What we want to do is build up a dictionary of N-grams, which are pairs, triplets or more (the N) of words that pop up in the training data, with the value being the number of times they showed up. After we have this dictionary, as a naive example we could actually predict sentences by just randomly choosing words within this dictionary and doing a weighted random sample of the connected words that are part of n-grams within the keys._\n",
    "\n",
    "_Lets see how far we can get with N-grams without outside resources._\n",
    "\n",
    "_We have a text file for [Pride and Prejudice from Project Gutenberg](https://www.gutenberg.org/ebooks/1342) stored as pg1342.txt in the same folder as our notebook, but also available online directly. Let's load the text to a string since it's only 701KB, which will fit in memory nowadays._\n",
    "\n",
    "_**Note** : If we wanted to be more memory efficient we should parse the text file on a line or character by character basis, storing as needed, etc._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "717572 , ï»¿The Project Gutenberg EBook of Pride and Prejudic ...\n"
     ]
    }
   ],
   "source": [
    "# Find the number links by looking on Project Gutenberg in the address bar for a book.\n",
    "books = {'Pride and Prejudice': '1342',\n",
    "         'Huckleberry Fin': '76',\n",
    "         'Sherlock Holmes': '1661'}\n",
    "\n",
    "book = books['Pride and Prejudice']\n",
    "\n",
    "# Load text from Project Gutenberg URL\n",
    "import requests\n",
    "url_template = 'https://www.gutenberg.org/cache/epub/%s/pg%s.txt'\n",
    "\n",
    "response = requests.get(url_template % (book, book), 'r')\n",
    "txt = response.text\n",
    "\n",
    "# See the number of characters and the first 50 characters to confirm it is there    \n",
    "print(len(txt), ',', txt[:50] , '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now lets split into words into a big list, splitting on anything non-alphanumeric [A-Za-z0-9] (as well as punctuation) and forcing everything lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "125897\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "words = re.split('[^A-Za-z]+', txt.lower())\n",
    "words = list(filter(None, words)) # Remove empty strings\n",
    "\n",
    "# Print length of list\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams generation\n",
    "From this we can now generate N-grams, lets start with a 1-gram, basically the set of all the words\n",
    "\n",
    "**Note** : One could use a dictionary instead of a set and keeping count of the occurances gives word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6528\n['withstood', 'burnt', 'spanish', 'commissioned', 'implies', 'cents', 'figures', 'family', 'guilt', 'camp', 'ventured', 'shades', 'effect', 'aught', 'form', 'conjecture', 'sides', 'noblest', 'independence', 'wavering']\n"
     ]
    }
   ],
   "source": [
    "# Create set of all unique words, this throws away any information about frequency however\n",
    "gram1 = set(words)\n",
    "\n",
    "print(len(gram1))\n",
    "\n",
    "# Print 20 elements of the set only\n",
    "print(list(gram1)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try and get the 2-gram now, which is pairs of words. Let's have a quick look to see the last 10 and how they look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "subscribe to\nto our\nour email\nemail newsletter\nnewsletter to\nto hear\nhear about\nabout new\nnew ebooks\n"
     ]
    }
   ],
   "source": [
    "# See the last 10 pairs\n",
    "for i in range(len(words)-10, len(words)-1):\n",
    "    print(words[i], words[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, seems good, lets get all word pairs, and then generate a set of unique pairs from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "125896\n55636\n[('had', 'often'), ('i', 'get'), ('gone', 'mr'), ('sit', 'by'), ('married', 'at'), ('any', 'public'), ('his', 'reason'), ('if', 'a'), ('her', 'story'), ('not', 'yours'), ('must', 'marry'), ('its', 'writer'), ('attention', 'which'), ('any', 'disrespect'), ('and', 'decisions'), ('precipitance', 'which'), ('ladyship', 'received'), ('has', 'the'), ('disposition', 'was'), ('upon', 'my')]\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
    "print(len(word_pairs))\n",
    "\n",
    "gram2 = set(word_pairs)\n",
    "print(len(gram2))\n",
    "\n",
    "# Print 20 elements from gram2\n",
    "print(list(gram2)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams Frequency\n",
    "Okay, that was fun, but this isn't enough, we need frequency if we want to have any sense of probabilities, which is what N-grams are about. Instead of using sets, lets create a dictionary with counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('the', 4507), ('to', 4243), ('of', 3730), ('and', 3658), ('her', 2225), ('i', 2070), ('a', 2012), ('in', 1937), ('was', 1847), ('she', 1710), ('that', 1594), ('it', 1550), ('not', 1450), ('you', 1428), ('he', 1339), ('his', 1271), ('be', 1260), ('as', 1192), ('had', 1177), ('with', 1100)]\n"
     ]
    }
   ],
   "source": [
    "gram1 = dict()\n",
    "\n",
    "# Populate 1-gram dictionary\n",
    "for word in words:\n",
    "    if word in gram1:\n",
    "        gram1[word] += 1\n",
    "    else:\n",
    "        gram1[word] = 1 # Start a new entry with 1 count since saw it for the first time.\n",
    "\n",
    "# Turn into a list of (word, count) sorted by count from most to least\n",
    "gram1 = sorted(gram1.items(), key=lambda item: -item[1])\n",
    "\n",
    "# Print top 20 most frequent words\n",
    "print([(word, freq) for word, freq in gram1[:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Pride and Prejudice, the words 'the', 'to', 'of', and 'and' were the top four most common words. Sounds about right, not too interesting yet, lets see what happens with 2-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(('of', 'the'), 491), (('to', 'be'), 445), (('in', 'the'), 397), (('i', 'am'), 303), (('mr', 'darcy'), 273), (('to', 'the'), 268), (('of', 'her'), 261), (('it', 'was'), 251), (('of', 'his'), 235), (('she', 'was'), 212), (('she', 'had'), 205), (('had', 'been'), 200), (('it', 'is'), 194), (('i', 'have'), 188), (('to', 'her'), 179), (('that', 'he'), 177), (('could', 'not'), 167), (('he', 'had'), 166), (('and', 'the'), 165), (('for', 'the'), 163)]\n"
     ]
    }
   ],
   "source": [
    "gram2 = dict()\n",
    "\n",
    "# Populate 2-gram dictionary\n",
    "for i in range(len(words)-1):\n",
    "    key = (words[i], words[i+1])\n",
    "    if key in gram2:\n",
    "        gram2[key] += 1\n",
    "    else:\n",
    "        gram2[key] = 1\n",
    "\n",
    "# Turn into a list of (word, count) sorted by count from most to least\n",
    "gram2 = sorted(gram2.items(), key=lambda item: -item[1])\n",
    "\n",
    "# Print top 20 most frequent words\n",
    "print([(word, freq) for word, freq in gram2[:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like \"of the\" and \"to be\" are the top two most common 2-grams, sounds good.\n",
    "\n",
    "##Â Next word prediction\n",
    "\n",
    "What can we do with this? Well lets see what happens if we take a random word from all the words, and build a sentence by just choosing the most common pair that has that word as it's start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "enough\n"
     ]
    }
   ],
   "source": [
    "start_word = words[int(len(words)/4)]\n",
    "print(start_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just went ahead and chose the word that appears $1/4$ of the way into words, random enough.\n",
    "\n",
    "Now in a loop, iterate through the frequency list (most frequent first) and see if it matches the first word in a pair, if so, the next word is the second element in the word pair, and continue with that word. Stop after N words or the list does not contain that word.\n",
    "\n",
    "**Note** : gram2 is a list that contains (key,value) where key is a word pair (first, second),\n",
    "           so you need element[0][0] for first word and element [0][1] for second word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start word: enough\n2-gram sentence: \" enough to be so much as to be so much as to be so much as to be so much \"\n"
     ]
    }
   ],
   "source": [
    "def get2GramSentence(word, n = 50):\n",
    "    words = []\n",
    "    for i in range(n):\n",
    "        words.append(word)\n",
    "        # Find Next word\n",
    "        word = next((element[0][1] for element in gram2 if element[0][0] == word), None)\n",
    "        if not word:\n",
    "            break\n",
    "    return ' '.join(words)\n",
    "\n",
    "word = start_word\n",
    "print(\"Start word:\", word)\n",
    "\n",
    "print('2-gram sentence: \"', get2GramSentence(word, 20), '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets stuck in a loop pretty much straight away. Not very interesting, try out other words and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start word: and\n2-gram sentence: \" and the whole party were to be so much as to be so much as to be so much as \"\nStart word: he\n2-gram sentence: \" he had been so much as to be so much as to be so much as to be so much \"\nStart word: she\n2-gram sentence: \" she was not be so much as to be so much as to be so much as to be so \"\nStart word: when\n2-gram sentence: \" when she was not be so much as to be so much as to be so much as to be \"\nStart word: john\n2-gram sentence: \" john with the whole party were to be so much as to be so much as to be so much \"\nStart word: never\n2-gram sentence: \" never be so much as to be so much as to be so much as to be so much as \"\nStart word: i\n2-gram sentence: \" i am sure i am sure i am sure i am sure i am sure i am sure i am \"\nStart word: how\n2-gram sentence: \" how much as to be so much as to be so much as to be so much as to be \"\n"
     ]
    }
   ],
   "source": [
    "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "    print(\"Start word:\", word)\n",
    "    print('2-gram sentence: \"', get2GramSentence(word, 20), '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted random choice based on frequency\n",
    "\n",
    "**This is our simple probabilistic MLE N-gram model**\n",
    "\n",
    "Same thing. Okay, lets randomly choose from the subset of all 2grams that matches the first word, using a weighted-probability based on counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def weighted_choice(choices):\n",
    "    total = sum(w for c, w in choices)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for c, w in choices:\n",
    "        if upto + w > r:\n",
    "            return c\n",
    "        upto += w\n",
    "    \n",
    "def get2GramSentenceRandom(word, n = 50):\n",
    "    words = []\n",
    "    for i in range(n):\n",
    "        words.append(word)\n",
    "        # Get all possible elements ((first word, second word), frequency)\n",
    "        choices = [element for element in gram2 if element[0][0] == word]\n",
    "        if not choices:\n",
    "            break\n",
    "        \n",
    "        # Choose a pair with weighted probability from the choice list\n",
    "        word = weighted_choice(choices)[1]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start word: and\n",
      "2-gram sentence: \" and jane mr collins has happened but you please her own and did sir such kinds of its contents as \"\n",
      "Start word: he\n",
      "2-gram sentence: \" he came only hope the days longer she had the company and then with no fresh source of chance in \"\n",
      "Start word: she\n",
      "2-gram sentence: \" she can you have just been a cheerful humour and good jokes did the hill my father did not unlikely \"\n",
      "Start word: when\n",
      "2-gram sentence: \" when i have suffered such behaviour in which they may incur by speaking to you could be kept them every \"\n",
      "Start word: john\n",
      "2-gram sentence: \" john with grave propriety of my dear eliza glancing over every thing and knowledge of civility to communicate and are \"\n",
      "Start word: never\n",
      "2-gram sentence: \" never in the easiness of shame and concluded had the gentleman and substitute a moment been to see your daughter \"\n",
      "Start word: i\n",
      "2-gram sentence: \" i believe us be likely to walk and lydia has neither at the attics are wasting your father cannot say \"\n",
      "Start word: how\n",
      "2-gram sentence: \" how many families who should have borne the gate in a reason and this agreement for mary could be happy \"\n"
     ]
    }
   ],
   "source": [
    "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "    print(\"Start word:\", word)\n",
    "    print('2-gram sentence: \"', get2GramSentenceRandom(word, 20), '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's way more interesting! Those are starting to look like sentences!\n",
    "\n",
    "Let's try a longer sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start word: it\n2-gram sentence: \" it rapid and pray tell the very wisely resolved to spend a disappointment at all wish at rosings certainly bestowed on her friends an airing would probably been a carriage as much better that you did not exactly defined she dreaded to know we can i can this is it \"\n"
     ]
    }
   ],
   "source": [
    "word = 'it'\n",
    "print(\"Start word:\", word)\n",
    "print('2-gram sentence: \"', get2GramSentenceRandom(word, 50), '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool, lets see what happens when we go to N-grams above 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Tri-grams and more\n",
    "Okay, let's create a Ngram generator that can let us make ngrams of arbitrary sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(('i', 'do', 'not'), 62), (('i', 'am', 'sure'), 62), (('project', 'gutenberg', 'tm'), 57), (('as', 'soon', 'as'), 55), (('she', 'could', 'not'), 50), (('that', 'he', 'had'), 37), (('it', 'would', 'be'), 34), (('in', 'the', 'world'), 34), (('i', 'am', 'not'), 32), (('the', 'project', 'gutenberg'), 31), (('i', 'dare', 'say'), 31), (('it', 'was', 'not'), 30), (('could', 'not', 'be'), 30), (('that', 'he', 'was'), 29), (('mr', 'darcy', 's'), 29), (('that', 'it', 'was'), 28), (('on', 'the', 'subject'), 28), (('of', 'mr', 'darcy'), 27), (('would', 'have', 'been'), 27), (('as', 'well', 'as'), 27)]\n"
     ]
    }
   ],
   "source": [
    "def generateNgram(inputwords,n=1):\n",
    "    gram = dict()\n",
    "    \n",
    "    # Some helpers to keep us crashing the PC for now\n",
    "    assert n > 0 and n < 100\n",
    "    \n",
    "    # Populate N-gram dictionary\n",
    "    for i in range(len(inputwords)-(n-1)):\n",
    "        key = tuple(inputwords[i:i+n])\n",
    "        if key in gram:\n",
    "            gram[key] += 1\n",
    "        else:\n",
    "            gram[key] = 1\n",
    "\n",
    "    # Turn into a list of (word, count) sorted by count from most to least\n",
    "    gram = sorted(gram.items(), key=lambda item: -item[1])\n",
    "    return gram\n",
    "\n",
    "trigram = generateNgram(words,3)\n",
    "# Print top 20 most frequent ngrams\n",
    "print(trigram[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generating 2-gram list...\n",
      "Done\n",
      "Start word: and\n",
      "2-gram sentence: \n",
      "\"and coming with she else dearly and illustrious good with that and have the had therefore i sorrow object speaking\"\n",
      "\n",
      "Start word: he\n",
      "2-gram sentence: \n",
      "\"he their test considering to the to i represented see and is which your say make away courage a parasol\"\n",
      "\n",
      "Start word: she\n",
      "2-gram sentence: \n",
      "\"she had forget resolved she if at would not many you miss will of no of so far thought guard\"\n",
      "\n",
      "Start word: when\n",
      "2-gram sentence: \n",
      "\"when so than there capable all him tm lucas it more to were welcomed unlink make but it whose left\"\n",
      "\n",
      "Start word: john\n",
      "2-gram sentence: \n",
      "\"john had ungenerous any discovered employed plead them way know there never i been natural up manners possible and for\"\n",
      "\n",
      "Start word: never\n",
      "2-gram sentence: \n",
      "\"never the after had extravagance speak which tete yourself had humiliating now it i return it but have a family\"\n",
      "\n",
      "Start word: i\n",
      "2-gram sentence: \n",
      "\"i could the had were gone by might surprise from that know or elizabeth now that a his cannot was\"\n",
      "\n",
      "Start word: how\n",
      "2-gram sentence: \n",
      "\"how must yours experienced fulfil affection unanswerable itself i of and sickly wish rose were they was observing near day\"\n",
      "\n",
      "Start word: country\n",
      "2-gram sentence: \n",
      "\"country the in she darcy were servant he detest dignified given and and without the an when through give half\"\n",
      "\n",
      "***************************************************************************\n",
      "Generating 3-gram list...\n",
      "Done\n",
      "Start word: and\n",
      "3-gram sentence: \n",
      "\"and felt nephew very not in you any at gratifying own and gratified and surprised returned up absence some in\"\n",
      "\n",
      "Start word: he\n",
      "3-gram sentence: \n",
      "\"he a instantly them dressing she or her would hours e not early allowed a silent and her with and\"\n",
      "\n",
      "Start word: she\n",
      "3-gram sentence: \n",
      "\"she of that not be works rosings improvement and a her be much own hope your not arts because favourable\"\n",
      "\n",
      "Start word: when\n",
      "3-gram sentence: \n",
      "\"when he drinking by character guessed and that trying continued the charlotte how the been course of no might been\"\n",
      "\n",
      "Start word: john\n",
      "3-gram sentence: \n",
      "\"john project vastly others was the without you can should bringing lose her matrimonial not air dislike with while constant\"\n",
      "\n",
      "Start word: never\n",
      "3-gram sentence: \n",
      "\"never the she to that play hunsford sisters conduct been so his person him manner jane terms addressed at of\"\n",
      "\n",
      "Start word: i\n",
      "3-gram sentence: \n",
      "\"i little no down of ask day you on he and him am for should as you in s from\"\n",
      "\n",
      "Start word: how\n",
      "3-gram sentence: \n",
      "\"how of him receive they to next after make herself her elopement hand and at were who usual meant from\"\n",
      "\n",
      "Start word: country\n",
      "3-gram sentence: \n",
      "\"country his according him the jane after was french of he from eyes her spend that was deletions it and\"\n",
      "\n",
      "***************************************************************************\n",
      "Generating 4-gram list...\n",
      "Done\n",
      "Start word: and\n",
      "4-gram sentence: \n",
      "\"and ideas the not made ten watering indulge sufficiently mr and said the not of real there room a twice\"\n",
      "\n",
      "Start word: he\n",
      "4-gram sentence: \n",
      "\"he as loo and help anything proud comprehend liability always letter a else design several and out created at to\"\n",
      "\n",
      "Start word: she\n",
      "4-gram sentence: \n",
      "\"she about mrs were be him observation s rather king at for be had way the meeting the less town\"\n",
      "\n",
      "Start word: when\n",
      "4-gram sentence: \n",
      "\"when but neighbour he will together first apologised does is in wished the i he stood his house often good\"\n",
      "\n",
      "Start word: john\n",
      "4-gram sentence: \n",
      "\"john project to that on after that you five was specified was trouble give of liberal hurst that however the\"\n",
      "\n",
      "Start word: never\n",
      "4-gram sentence: \n",
      "\"never could gardiner of be sitting you in affect of am so this there world before softened love elizabeth me\"\n",
      "\n",
      "Start word: i\n",
      "4-gram sentence: \n",
      "\"i see refusing the man bourgh nothing conversation to to did had every proved i that he too to where\"\n",
      "\n",
      "Start word: how\n",
      "4-gram sentence: \n",
      "\"how returning had them not its and mean mr long inspired can considered his th she wild assurances all any\"\n",
      "\n",
      "Start word: country\n",
      "4-gram sentence: \n",
      "\"country replied he should her though him such serious letters unsettled her lady for i to been more had blessing\"\n",
      "\n",
      "***************************************************************************\n",
      "Generating 5-gram list...\n",
      "Done\n",
      "Start word: and\n",
      "5-gram sentence: \n",
      "\"and the blind mean character another quarter never his lose of just to mr may have anxiety it or rose\"\n",
      "\n",
      "Start word: he\n",
      "5-gram sentence: \n",
      "\"he living word repeating after was but is elegance but ever to me uncle they but her forgotten to even\"\n",
      "\n",
      "Start word: she\n",
      "5-gram sentence: \n",
      "\"she mr careful hurst are i as its niece s say music not had hope the minutes importance to the\"\n",
      "\n",
      "Start word: when\n",
      "5-gram sentence: \n",
      "\"when impossible always excessively am character she dwelling servant fitzwilliam seen is and to every rank she happiness without and\"\n",
      "\n",
      "Start word: john\n",
      "5-gram sentence: \n",
      "\"john had redistribution it as them a de saw and any said could spent longbourn i discharged young was the\"\n",
      "\n",
      "Start word: never\n",
      "5-gram sentence: \n",
      "\"never cousins of to hesitated lucas in her the dear him of know which mrs to willing week was the\"\n",
      "\n",
      "Start word: i\n",
      "5-gram sentence: \n",
      "\"i she ago to advantage comfort what does but two i in began and i fitted not to glad as\"\n",
      "\n",
      "Start word: how\n",
      "5-gram sentence: \n",
      "\"how and longed must ladies and way stay subject of pretty and she properly no have was of to mr\"\n",
      "\n",
      "Start word: country\n",
      "5-gram sentence: \n",
      "\"country no natured contrary good reflection no and but to all if heart join be as name often other a\"\n",
      "\n",
      "***************************************************************************\n",
      "Generating 6-gram list...\n",
      "Done\n",
      "Start word: and\n",
      "6-gram sentence: \n",
      "\"and notice have from associated and be father in were mend she might to which on had even do early\"\n",
      "\n",
      "Start word: he\n",
      "6-gram sentence: \n",
      "\"he herself consent prepare be mr they but mr such of that contemptuously little intimidate have as is true to\"\n",
      "\n",
      "Start word: she\n",
      "6-gram sentence: \n",
      "\"she at called if must then conceited time she her me it say time she no herself will was courted\"\n",
      "\n",
      "Start word: when\n",
      "6-gram sentence: \n",
      "\"when why been be the binary and on sister disadvantages off my you family you she quite very from and\"\n",
      "\n",
      "Start word: john\n",
      "6-gram sentence: \n",
      "\"john had and the tranquil the to experienced me acquaintance soon to as mother as pleasure not heedless me it\"\n",
      "\n",
      "Start word: never\n",
      "6-gram sentence: \n",
      "\"never unconcerned are before determine in so he kind otherwise mother she i it do let human when be i\"\n",
      "\n",
      "Start word: i\n",
      "6-gram sentence: \n",
      "\"i employment road encourage doing of and as i to gentle of wrong to ever to for did bless elizabeth\"\n",
      "\n",
      "Start word: how\n",
      "6-gram sentence: \n",
      "\"how must and the in bennet it my lady and and her warm should this this a about of to\"\n",
      "\n",
      "Start word: country\n",
      "6-gram sentence: \n",
      "\"country could she it home minutely my that will wrong greatest a soon and with has that her me would\"\n",
      "\n",
      "***************************************************************************\n",
      "Generating 7-gram list...\n",
      "Done\n",
      "Start word: and\n",
      "7-gram sentence: \n",
      "\"and that her the in about i he were because dared of to bennet by this to could nothing why\"\n",
      "\n",
      "Start word: he\n",
      "7-gram sentence: \n",
      "\"he at no to other have was catherine or promises she no he me beyond housekeeper judgement colouring for thing\"\n",
      "\n",
      "Start word: she\n",
      "7-gram sentence: \n",
      "\"she concluded part seen mr to settled the of above the the and when determine the s my again jane\"\n",
      "\n",
      "Start word: when\n",
      "7-gram sentence: \n",
      "\"when once as to nor was ago studious unknown she that the of imposed into guess himself yes to unhappy\"\n",
      "\n",
      "Start word: john\n",
      "7-gram sentence: \n",
      "\"john had s and an a of it your she your next no objection were youth a on why an\"\n",
      "\n",
      "Start word: never\n",
      "7-gram sentence: \n",
      "\"never still look he i denial to was is give the felt a miss she always excessively am exactly come\"\n",
      "\n",
      "Start word: i\n",
      "7-gram sentence: \n",
      "\"i be thing solemnity and answer they possible rest wretchedness descended seen is picture country but again the that self\"\n",
      "\n",
      "Start word: how\n",
      "7-gram sentence: \n",
      "\"how an striking implicit how i to me be road seat marriage and him for the see been neither grow\"\n",
      "\n",
      "Start word: country\n",
      "7-gram sentence: \n",
      "\"country and her you the s console to have was your had this an you a long walk statements and\"\n",
      "\n",
      "***************************************************************************\n",
      "Generating 8-gram list...\n",
      "Done\n",
      "Start word: and\n",
      "8-gram sentence: \n",
      "\"and grievances your do own will many be nothing minded am was family not you chance got satisfaction with but\"\n",
      "\n",
      "Start word: he\n",
      "8-gram sentence: \n",
      "\"he must of given pardon may of t admiration to project were the know using will time here all made\"\n",
      "\n",
      "Start word: she\n",
      "8-gram sentence: \n",
      "\"she them to likewise charlotte the as bennet and not being away elizabeth a young had the am she go\"\n",
      "\n",
      "Start word: when\n",
      "8-gram sentence: \n",
      "\"when them in partner acquaintance near great wounding principal her the inform be to perfectly the is is be but\"\n",
      "\n",
      "Start word: john\n",
      "8-gram sentence: \n",
      "\"john had me could they soon derives in i so their of much all ladies direct rest mr in no\"\n",
      "\n",
      "Start word: never\n",
      "8-gram sentence: \n",
      "\"never receive seemed be i herself making to high i determine my them party or of only struck now her\"\n",
      "\n",
      "Start word: i\n",
      "8-gram sentence: \n",
      "\"i the passed for which on to these declaring soon her thus any imposed of i now does have darcy\"\n",
      "\n",
      "Start word: how\n",
      "8-gram sentence: \n",
      "\"how the merry less one impatience only when the out i a former numerous i is be wickham i few\"\n",
      "\n",
      "Start word: country\n",
      "8-gram sentence: \n",
      "\"country nature care was might happened is company the she you spirits once is an impelled her quick to meeting\"\n",
      "\n",
      "***************************************************************************\n",
      "Generating 9-gram list...\n",
      "Done\n",
      "Start word: and\n",
      "9-gram sentence: \n",
      "\"and of not have taking my our said forcibly furniture her you cried imprudent salutation most of he twenty easily\"\n",
      "\n",
      "Start word: he\n",
      "9-gram sentence: \n",
      "\"he her for and of it on of as then arrival to alarm countenance man addressed appeared respectability to upon\"\n",
      "\n",
      "Start word: she\n",
      "9-gram sentence: \n",
      "\"she a hopes related us who bennets at expression intended listened which s but world days was assistance a sanction\"\n",
      "\n",
      "Start word: when\n",
      "9-gram sentence: \n",
      "\"when very but to other for in who home they be certainly composure certainly that creatures father did younger reports\"\n",
      "\n",
      "Start word: john\n",
      "9-gram sentence: \n",
      "\"john had latter she me give stay wickham left in so and before having neither was beauty and and great\"\n",
      "\n",
      "Start word: never\n",
      "9-gram sentence: \n",
      "\"never of me where to their wide give of my they others bourgh volunteers stay bennet that darcy then and\"\n",
      "\n",
      "Start word: i\n",
      "9-gram sentence: \n",
      "\"i about my own utmost i and particularly when imposing s there of the for and next salt go conviction\"\n",
      "\n",
      "Start word: how\n",
      "9-gram sentence: \n",
      "\"how of hertfordshire love totally be aunt last of as exposing aside tolerable from elizabeth something go and rosings recollect\"\n",
      "\n",
      "Start word: country\n",
      "9-gram sentence: \n",
      "\"country small after had year all to daughter in house country how indifference it so could mistaken nobody only newcastle\"\n",
      "\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Added inputwords as first parameter in order to used in in part2\n",
    "def getNGramSentenceRandom(gram, word, n = 50):\n",
    "    words = []\n",
    "    for i in range(n):\n",
    "        words.append(word)\n",
    "        # Get all possible elements ((first word, second word), frequency)\n",
    "        choices = [element for element in gram if element[0][0] == word]\n",
    "        if not choices:\n",
    "            break\n",
    "        \n",
    "        # Choose a pair with weighted probability from the choice list\n",
    "        word = weighted_choice(choices)[1]\n",
    "    return ' '.join(words)\n",
    "\n",
    "for n in range(2,10):\n",
    "    # Generate ngram list\n",
    "    print(f\"Generating {n}-gram list...\")\n",
    "    ngram = generateNgram(words,n)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    # Try out a bunch of sentences\n",
    "    for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how','country']:\n",
    "        print(\"Start word:\", word)\n",
    "        print(f'{n}-gram sentence: \\n\"{getNGramSentenceRandom(ngram, word, 20)}\"')\n",
    "        print()\n",
    "        \n",
    "    print('***************************************************************************')"
   ]
  },
  {
   "source": [
    "# Continue Assignment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Part 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from operator import itemgetter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countNGram(frequency):\n",
    "    count ={}\n",
    "    \n",
    "    for (key,fvalue) in reversed(sorted(frequency.items(), key = itemgetter(1))):\n",
    "        count[key] = fvalue\n",
    " \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNGramList(count_dict):\n",
    "    l = count_dict \n",
    "    l=list(l.values())\n",
    "    l.sort()\n",
    "\n",
    "    d={}\n",
    "    for i in range(len(l)):\n",
    "        count =1\n",
    "        for j in range(i,0,-1):\n",
    "            if l[i] == l[j-1]:\n",
    "                count = count + 1\n",
    "            else:\n",
    "                break \n",
    "    \n",
    "        d[l[i]] = count\n",
    "    return (d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class goodTuring:\n",
    "    def __init__(self,Ncount,Ucount): #dictionary of unigram and 4gram count\n",
    "        self.nGramCount = Ncount.copy()\n",
    "        self.UniGramCount = Ucount.copy()\n",
    "        \n",
    "        self.ndictN = getNGramList(self.nGramCount)\n",
    "        self.UnidictN = getNGramList(self.UniGramCount)\n",
    "        \n",
    "        self.keysn,self.valuesn = list(self.ndictN.keys()),list(self.ndictN.values())    \n",
    "        self.keysuni,self.valuesuni = list(self.UnidictN.keys()),list(self.UnidictN.values()) \n",
    "        \n",
    "        self.NewCountN()\n",
    "        self.NewCountuni()\n",
    "\n",
    "    def NewCountN(self):\n",
    "               \n",
    "        for i in self.nGramCount.keys():\n",
    "            try:\n",
    "                self.nGramCount[i] = (self.nGramCount[i]+1)*(self.ndictN[self.nGramCount[i]+1]/self.ndictN[self.nGramCount[i]])\n",
    "            except:\n",
    "                intr= np.interp(float(self.nGramCount[i]),[float(j) for j in self.keysn],[float(k) for k in self.valuesn])\n",
    "\n",
    "                self.nGramCount[i] = (self.nGramCount[i]+1)*(intr/(self.ndictN[self.nGramCount[i]]))\n",
    "\n",
    "        return self.nGramCount \n",
    "\n",
    "    def NewCountuni(self):\n",
    "               \n",
    "        for i in self.UniGramCount.keys():\n",
    "            try:\n",
    "                self.UniGramCount[i] = (self.UniGramCount[i]+1)*(self.UnidictN[self.UniGramCount[i]+1]/self.UnidictN[self.UniGramCount[i]])\n",
    "            except:\n",
    "                intr= np.interp(float(self.UniGramCount[i]),[float(j) for j in self.keysuni],[float(k) for k in self.valuesuni])\n",
    "\n",
    "                self.UniGramCount[i] = (self.UniGramCount[i]+1)*(intr/(self.UnidictN[self.UniGramCount[i]]))\n",
    "\n",
    "\n",
    "        return self.UniGramCount \n",
    "    \n",
    "    def Probability(self,a,b,c,d, tokenNgram): \n",
    "        ngram = d +\" \"+ c +\" \"+ b +\" \"+ a\n",
    "        if ngram not in self.nGramCount.keys():\n",
    "            return (self.ndictN[1]/len(tokenNgram))\n",
    "        \n",
    "        return self.nGramCount[ngram]/self.UniGramCount[b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perplexity(data,bigramLst,model):\n",
    "    log_perplxty = 0\n",
    "    for i in data:\n",
    "        log_prob = 0\n",
    "        a = generateNgram([i],4)\n",
    "        for i in a:\n",
    "            print(i)\n",
    "            new = model.Probability(i.split(\" \")[3],i.split(\" \")[2],i.split(\" \")[1],i.split(\" \")[0],bigramLst)\n",
    "            log_prob = log_prob + math.log(new)\n",
    "            \n",
    "        log_perplxty = log_perplxty+log_prob\n",
    "        log_perplxty = (-1/len(data))*(log_perplxty)\n",
    "       \n",
    "    return (math.exp(log_perplxty) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split words into train-test\r\n",
    "random.shuffle(words)\r\n",
    "\r\n",
    "train = words[:int(len(words)*0.8)]#80%\r\n",
    "test= words[int(len(words)*0.8):]#20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramLst = generateNgram(train,1)\n",
    "quadgramLst= generateNgram(train,4)\n",
    "fr   =  nltk.FreqDist(quadgramLst)\n",
    "feq  =  nltk.FreqDist(unigramLst)\n",
    "count_unigram = countNGram(feq)\n",
    "count_quadgram = countNGram(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split text into sentences using nltk\n",
    "sentences = nltk.sent_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sentences_clean = [p for p in sentences if p not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Most common n-grams without stopword removal and without add-1 smoothing: \n\n\nMost common fourgrams:  [((',', \"''\", 'said', 'she'), 41), ((',', \"''\", 'said', 'elizabeth'), 37), ((\"''\", 'said', 'she', ','), 34), ((',', 'i', 'am', 'sure'), 29), ((',', \"''\", 'said', 'he'), 28)]\n"
     ]
    }
   ],
   "source": [
    "#Create the language models on the training corpus (Fourgram)\n",
    "\n",
    "from nltk.util import ngrams\n",
    "unigram=[]\n",
    "fourgram=[]\n",
    "tokenized_text = []\n",
    "for sentence in sentences_clean:\n",
    "    sentence = sentence.lower()\n",
    "    sequence = word_tokenize(sentence) \n",
    "    for word in sequence:\n",
    "        if (word =='.'):\n",
    "            sequence.remove(word) \n",
    "        else:\n",
    "            unigram.append(word)\n",
    "    tokenized_text.append(sequence) \n",
    "    fourgram.extend(list(ngrams(sequence, 4)))\n",
    "\n",
    "def removal(x):     \n",
    "    y = []\n",
    "    for pair in x:\n",
    "        count = 0\n",
    "        for word in pair:\n",
    "            if word in stop_words:\n",
    "                count = count or 0\n",
    "            else:\n",
    "                count = count or 1\n",
    "        if (count==1):\n",
    "            y.append(pair)\n",
    "    return(y)        \n",
    "fourgram = removal(fourgram)\n",
    "freq_four = nltk.FreqDist(fourgram)\n",
    "print(\"Most common n-grams without stopword removal and without add-1 smoothing: \\n\")\n",
    "print (\"\\nMost common fourgrams: \", freq_four.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary where each element is a list corresponding to a particular n-gram, and store every word and its associated probability as elements of the list. Add-1 smoothing is performed considering all the unique words in the tokenized text of our dataset as the vocabulary.\n",
    "\n",
    "#Add-1 smoothing is performed here.\n",
    "            \n",
    "ngrams_all = {1:[], 2:[], 3:[], 4:[]}\n",
    "for i in range(4):\n",
    "    for each in tokenized_text:\n",
    "        for j in ngrams(each, i+1):\n",
    "            ngrams_all[i+1].append(j);\n",
    "ngrams_voc = {1:set([]), 2:set([]), 3:set([]), 4:set([])}\n",
    "for i in range(4):\n",
    "    for gram in ngrams_all[i+1]:\n",
    "        if gram not in ngrams_voc[i+1]:\n",
    "            ngrams_voc[i+1].add(gram)\n",
    "total_ngrams = {1:-1, 2:-1, 3:-1, 4:-1}\n",
    "total_voc = {1:-1, 2:-1, 3:-1, 4:-1}\n",
    "for i in range(4):\n",
    "    total_ngrams[i+1] = len(ngrams_all[i+1])\n",
    "    total_voc[i+1] = len(ngrams_voc[i+1])                       \n",
    "    \n",
    "ngrams_prob = {1:[], 2:[], 3:[], 4:[]}\n",
    "for i in range(4):\n",
    "    for ngram in ngrams_voc[i+1]:\n",
    "        tlist = [ngram]\n",
    "        tlist.append(ngrams_all[i+1].count(ngram))\n",
    "        ngrams_prob[i+1].append(tlist)\n",
    "    \n",
    "for i in range(4):\n",
    "    for ngram in ngrams_prob[i+1]:\n",
    "        ngram[-1] = (ngram[-1]+1)/(total_ngrams[i+1]+total_voc[i+1])             #add-1 smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Most common n-grams without stopword removal and with add-1 smoothing: \n\n\nMost common fourgrams:  [[(',', \"''\", 'said', 'she'), 0.0001755082426192516], [(',', \"''\", 'said', 'elizabeth'), 0.00015879317189360857], [(\"''\", 'said', 'she', ','), 0.0001462568688493763], [(',', 'i', 'am', 'sure'), 0.00012536303044232257], [(',', \"''\", 'said', 'he'), 0.00012118426276091181], [('said', 'she', ',', '``'), 0.00011282672739809031], [(\"''\", 'said', 'he', ','), 0.0001044691920352688], [(',', 'my', 'dear', ','), 9.193288899103655e-05], [(\"''\", 'said', 'elizabeth', ','), 8.77541213096258e-05], [(',', 'she', 'could', 'not'), 8.357535362821503e-05], [('i', 'do', 'not', 'know'), 8.357535362821503e-05], [('*', '*', '*', '*'), 8.357535362821503e-05], [(',', \"''\", 'replied', 'elizabeth'), 7.939658594680428e-05], [(',', \"''\", 'said', 'her'), 7.939658594680428e-05], [('said', 'he', ',', '``'), 7.521781826539353e-05], [(',', \"''\", 'said', 'mrs.'), 7.521781826539353e-05], [(',', 'i', 'suppose', ','), 7.103905058398278e-05], [('``', 'i', 'do', 'not'), 7.103905058398278e-05], [('at', 'the', 'same', 'time'), 7.103905058398278e-05], [('i', 'am', 'sure', ','), 6.686028290257204e-05]]\n"
     ]
    }
   ],
   "source": [
    "#Prints top 10 unigram, bigram, trigram, fourgram after smoothing\n",
    "print(\"Most common n-grams without stopword removal and with add-1 smoothing: \\n\")\n",
    "for i in range(4):\n",
    "    ngrams_prob[i+1] = sorted(ngrams_prob[i+1], key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "print (\"\\nMost common fourgrams: \", str(ngrams_prob[4][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute using good-turing\n",
    "probTuring = goodTuring(count_quadgram,count_unigram)"
   ]
  },
  {
   "source": [
    "# Part 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "594916 , ï»¿Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doy ...\n"
     ]
    }
   ],
   "source": [
    "#Select Sherlock Holmes book and get it directly from url\n",
    "book = books['Sherlock Holmes']\n",
    "\n",
    "# Load text from Project Sherlock Holmes URL\n",
    "import requests\n",
    "url_template = 'https://www.gutenberg.org/cache/epub/%s/pg%s.txt'\n",
    "\n",
    "response = requests.get(url_template % (book, book), 'r')\n",
    "txt2 = response.text\n",
    "\n",
    "print(len(txt2), ',', txt2[:75] , '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "109000\n"
     ]
    }
   ],
   "source": [
    "#Get words of selected text\n",
    "import re\n",
    "words_sh = re.split('[^A-Za-z]+', txt2.lower())\n",
    "words_sh = list(filter(None, words_sh)) \n",
    "\n",
    "print(len(words_sh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle words and split to train and test\n",
    "random.shuffle(words_sh)\n",
    "\n",
    "train_sh = words_sh[:int(len(words_sh)*0.8)]#80%\n",
    "test_sh= words_sh[int(len(words_sh)*0.8):]#20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "21800"
      ]
     },
     "metadata": {},
     "execution_count": 284
    }
   ],
   "source": [
    "len(test_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourGram = generateNgram(train_sh,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generating 4-gram list...\n",
      "Done\n",
      "Start word: and\n",
      "4-gram sentence: \n",
      "\"and white the my which facts man of order do glance recent your it the the had that and you\"\n",
      "\n",
      "Start word: he\n",
      "4-gram sentence: \n",
      "\"he proud seemed books see would his incident there the for me i seeing up as in some lady thief\"\n",
      "\n",
      "Start word: she\n",
      "4-gram sentence: \n",
      "\"she to swaying yes that now father with pounds even america your it old impatient on for press until horse\"\n",
      "\n",
      "Start word: when\n",
      "4-gram sentence: \n",
      "\"when that heard occupy america your you exacted man the made room idea explained i in for point departure confederates\"\n",
      "\n",
      "Start word: john\n",
      "4-gram sentence: \n",
      "\"john passed governesses to merest of lock no centre park by a you any but for a o brass you\"\n",
      "\n",
      "Start word: never\n",
      "4-gram sentence: \n",
      "\"never a was i paper cleaver the or about lad conditions to roof it written said hair had until rearranging\"\n",
      "\n",
      "Start word: i\n",
      "4-gram sentence: \n",
      "\"i clearly you singularly at have do the yes flicking gentle she acquirement dissatisfied i because here mere how to\"\n",
      "\n",
      "Start word: how\n",
      "4-gram sentence: \n",
      "\"how that to bye shop we allied well dared slipped the when hot we which beryl mccarthy dropped years dispose\"\n",
      "\n",
      "Start word: country\n",
      "4-gram sentence: \n",
      "\"country a and the she house ve a lord s repeat with interesting is more however so in is those\"\n",
      "\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for n in range(4,5):\n",
    "    # Generate ngram list\n",
    "    print(f\"Generating {n}-gram list...\")\n",
    "    ngram = generateNgram(words_sh,n)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    # Try out some of sentences\n",
    "    for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how','country']:\n",
    "        print(\"Start word:\", word)\n",
    "        print(f'{n}-gram sentence: \\n\"{getNGramSentenceRandom(ngram, word, 20)}\"')\n",
    "        print()\n",
    "        \n",
    "    print('***************************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(('and', 'and', 'the', 'i'), 2), (('the', 'a', 'and', 'been'), 2), (('to', 'the', 'he', 'would'), 2), (('a', 'had', 'the', 'and'), 2), (('the', 'it', 'which', 'the'), 2), (('of', 'to', 'the', 'a'), 2), (('to', 'i', 'from', 'was'), 2), (('the', 'no', 'that', 'and'), 2), (('it', 'the', 'the', 'i'), 2), (('i', 'and', 'the', 'the'), 2)]\n"
     ]
    }
   ],
   "source": [
    "#list containing n-gram\n",
    "quadgramLst= generateNgram(train_sh,4)\n",
    "print (quadgramLst[:10])\n",
    "#  query for count of unigram\n",
    "fr   =  nltk.FreqDist(quadgramLst)\n",
    "\n",
    "count_4gram = countNGram(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "probTuring = goodTuring(count_4gram,count_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate NGram model for test set\n",
    "fourGramLst = generateNgram(test,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(('her', 'in', 'very', 'on'), 2),\n",
       " (('to', 'was', 'ask', 'per'), 1),\n",
       " (('was', 'ask', 'per', 'jenkinson'), 1),\n",
       " (('ask', 'per', 'jenkinson', 'delivered'), 1),\n",
       " (('per', 'jenkinson', 'delivered', 'than'), 1)]"
      ]
     },
     "metadata": {},
     "execution_count": 295
    }
   ],
   "source": [
    "fourGramLst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Good Turing Result: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Perplexity\n",
    "per = Perplexity(fourGramLst,quadgramLst,probTuring)\n",
    "print (\"Good Turing Result:\",per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Good turing performs better than add one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}